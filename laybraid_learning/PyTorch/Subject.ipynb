{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FashionMNIST\n",
    "\n",
    "![gif](assets/embedding.gif)\n",
    "\n",
    "\n",
    "You should solve the FashionMNIST problems and be able to recognize clothes thanks to AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\tIf necessary you can un-comment the next line and run it to install this notebook depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: The directory '/Users/clementloeuillet/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001B[0m\n",
      "Requirement already satisfied: torch in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (1.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (0.8.0a0)\n",
      "Requirement already satisfied: numpy in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (1.19.2)\n",
      "Requirement already satisfied: matplotlib in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/clementloeuillet/opt/anaconda3/envs/jupyter/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform each image into tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Set the training loader\n",
    "train_data = datasets.FashionMNIST('../data', train=True, download=True, transform=transform)\n",
    "# Set the testing loader\n",
    "test_data = datasets.FashionMNIST('../data', train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The dataset\n",
    "\n",
    " - `train_data` is only for training your model.\n",
    " - `test_data`  is only for testing your model.\n",
    "\n",
    "## Shape\n",
    "\n",
    "```python\n",
    "train_data = [\n",
    "\t[image], [label] # 1st example\n",
    "\t[image], [label] # 2nd example\n",
    "\t[image], [label] # 3rd example\n",
    "\t[image], [label] # 4th example\n",
    "\t... # 4th example\n",
    "]\n",
    "```\n",
    "\n",
    "There's 60 000 examples in the train set, and 10 000 in the test set\n",
    "\n",
    "### Image\n",
    "\n",
    "An image is 28*28 in black & white \n",
    "\n",
    "You can access the first image this way:\n",
    "\n",
    "```python\n",
    "image_0 = data[0][0]\n",
    "```\n",
    "\n",
    "### Label\n",
    "\n",
    "A label is an integer between 0 and 9 (included) \n",
    "\n",
    "You can access the first label this way:\n",
    "\n",
    "```python\n",
    "label_0 = data[0][0]\n",
    "```\n",
    "\n",
    "Here is the detail of each label\n",
    "\n",
    "| Label | Description |\n",
    "|:-|:-:|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Train len : \" + str(len(test_data)))\n",
    "print(\"Test len : \" + str(len(train_data)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Image Size: \" + str(train_data[0][0].shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_dic = {\n",
    "\t0: \"T-shirt/top\",\n",
    "\t1: \"Trouser\",\n",
    "\t2 : \"Pullover \",\n",
    "\t3 : \"Dress \",\n",
    "\t4 : \"Coat \",\n",
    "\t5 : \"Sandal \",\n",
    "\t6 : \"Shirt \",\n",
    "\t7 : \"Sneaker \",\n",
    "\t8 : \"Bag \",\n",
    "\t9 : \"Ankle boot\"\n",
    "}\n",
    "\n",
    "def plot_one_example(example):\n",
    "\tplt.imshow(example[0].view(28, 28), cmap=\"gray\")\n",
    "\tplt.title(\"Label class {}: {}\".format(example[1], label_dic[example[1]]))\n",
    "\tplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_one_example(train_data[0])\n",
    "plot_one_example(train_data[-1])\n",
    "plot_one_example(train_data[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to train a model to predict the label of an image !\n",
    "\n",
    "# ðŸš€ Good Luck "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
    "        self.linear1 = nn.Linear(4 * 4, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = x.reshape(-1, 4 * 4)\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCH = 3\n",
    "\n",
    "model = MyModel()\n",
    "loss_fonct = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(EPOCH): #training\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        output = model.forward(images)\n",
    "        loss = loss_fonct(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "total, correct = 0, 0\n",
    "for image, label in test_data: #testing\n",
    "    output = model.forward(image.reshape(1, 1, 28, 28))\n",
    "    if (output.argmax(dim=1).item() == label):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Accuracy:\", correct / total * 100 ,\"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7752, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7617, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7920, grad_fn=<NllLossBackward>)\n",
      "Accuracy: 74.92999999999999 %\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 3\n",
    "\n",
    "model = MyModel()\n",
    "loss_fonct = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(EPOCH): #training\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        output = model.forward(images)\n",
    "        loss = loss_fonct(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "total, correct = 0, 0\n",
    "for image, label in test_data: #testing\n",
    "    output = model.forward(image.reshape(1, 1, 28, 28))\n",
    "    if (output.argmax(dim=1).item() == label):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Accuracy:\", correct / total * 100 ,\"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}